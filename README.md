# household-classification-ml
Machine Learning project: predicting number of children using socio-demographic data (8,400 observations)
# ğŸ  Household Classification - Predicting Number of Children

[![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://www.python.org/)
[![Scikit-learn](https://img.shields.io/badge/Scikit--learn-1.3.0-orange.svg)](https://scikit-learn.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

## ğŸ“‹ Project Overview

Machine learning classification project to predict the number of children in households based on **8,400 socio-demographic observations**.

This project demonstrates end-to-end data science workflow from exploratory data analysis to model deployment.

## ğŸ¯ Business Objective

Build and compare classification models to identify key factors influencing family size decisions, providing actionable insights for:
- Social policy targeting
- Demographic forecasting
- Resource allocation planning

## ğŸ“Š Dataset

- **Size**: 8,400 observations
- **Features**: Socio-demographic variables including:
  - Income level
  - Education
  - Occupation
  - Geographic location
  - Age
  - Marital status
- **Target Variable**: Number of children (multi-class classification)

## ğŸ› ï¸ Tech Stack

- **Language**: Python 3.10+
- **Libraries**: 
  - Data Processing: Pandas, NumPy
  - Machine Learning: Scikit-learn
  - Visualization: Matplotlib, Seaborn
  - Statistical Testing: SciPy
- **Environment**: Jupyter Notebook

## ğŸ” Methodology

### 1. Data Preprocessing
- Missing values imputation
- Categorical variables encoding (One-Hot, Label Encoding)
- Feature scaling and normalization
- Train-test split (80/20)

### 2. Exploratory Data Analysis (EDA)
- Descriptive statistics
- Distribution analysis
- Correlation heatmaps
- Statistical hypothesis testing:
  - Chi-square test
  - ANOVA
  - MANOVA
  - Normality tests

### 3. Feature Engineering
- Variable selection based on correlation and domain knowledge
- Feature importance analysis
- Dimensionality reduction exploration (PCA)

### 4. Model Training & Evaluation

**Models Tested:**
- Logistic Regression (baseline)
- Decision Tree
- Random Forest (best performer)

**Evaluation Metrics:**
- âœ… **Accuracy**: 75%
- âœ… **F1-Score**: 0.72
- Confusion Matrix
- Precision & Recall per class

## ğŸ“ˆ Key Results

### Model Performance Comparison

| Model | Accuracy | F1-Score | Training Time |
|-------|----------|----------|---------------|
| Logistic Regression | 68% | 0.65 | 2s |
| Decision Tree | 71% | 0.68 | 5s |
| **Random Forest** | **75%** | **0.72** | 15s |

### Top 3 Predictive Features
1. **Income Level** (importance: 0.35)
2. **Education** (importance: 0.28)
3. **Age** (importance: 0.22)

### Insights
- Higher education correlates with fewer children
- Income shows non-linear relationship with family size
- Geographic location is less predictive than expected

## ğŸš€ How to Run

### Prerequisites
```bash
# Clone the repository
git clone https://github.com/Litimjihen/household-classification-ml.git
cd household-classification-ml

# Install dependencies
pip install -r requirements.txt
```

### Run Analysis
```bash
# Launch Jupyter Notebook
jupyter notebook

# Open notebooks in order:
# 1. notebooks/01_EDA.ipynb
# 2. notebooks/02_preprocessing.ipynb
# 3. notebooks/03_modeling.ipynb
```

## ğŸ“ Project Structure
```
household-classification-ml/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/              # Original dataset
â”‚   â””â”€â”€ processed/        # Cleaned data
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_EDA.ipynb           # Exploratory Data Analysis
â”‚   â”œâ”€â”€ 02_preprocessing.ipynb # Data cleaning & feature engineering
â”‚   â””â”€â”€ 03_modeling.ipynb      # Model training & evaluation
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_preprocessing.py  # Data cleaning functions
â”‚   â”œâ”€â”€ feature_engineering.py # Feature creation utilities
â”‚   â””â”€â”€ model_training.py      # Model training pipeline
â”‚
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ figures/          # Visualizations
â”‚   â””â”€â”€ models/           # Saved models (.pkl)
â”‚
â”œâ”€â”€ requirements.txt      # Python dependencies
â”œâ”€â”€ README.md            # Project documentation
â”œâ”€â”€ .gitignore           # Git ignore rules
â””â”€â”€ LICENSE              # MIT License
```

## ğŸ”® Future Improvements

- [ ] Hyperparameter tuning with GridSearchCV/RandomizedSearchCV
- [ ] Test ensemble methods (XGBoost, LightGBM, CatBoost)
- [ ] Implement SMOTE for class imbalance handling
- [ ] Deploy model as REST API (FastAPI)
- [ ] Add SHAP values for model interpretability
- [ ] Create interactive dashboard with Streamlit

## ğŸ“š Learnings

This project reinforced my understanding of:
- Complete ML workflow from raw data to production-ready model
- Statistical testing for data validation
- Feature engineering impact on model performance
- Importance of model interpretability in business contexts

## ğŸ‘¤ Author

**Jihen LITIM**  
MSc Artificial Intelligence Student | Former Embedded Systems Engineer  

ğŸ“§ [jihen.litim@aivancity.education](mailto:jihen.litim@aivancity.education)  
ğŸ”— [LinkedIn](https://www.linkedin.com/in/votre-profil)  
ğŸ™ [GitHub](https://github.com/Litimjihen)

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

â­ **If you found this project helpful, please consider giving it a star!**
